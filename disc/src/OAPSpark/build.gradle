apply plugin: "scala"

tasks.withType(ScalaCompile) {
    options.encoding = "UTF-8"
}

version = "0.0.1"


repositories {
    mavenLocal()
    mavenCentral()
}

ext.ver = [
        scala  : "2.11",
        hadoop : "2.7.3",
        spark  : "2.3.3",
        kafka  : "0.10.1",
        jackson: "2.7.8"
]

def spark = { module ->
    "org.apache.spark:spark-${module}_${ver.scala}:${ver.spark}"
}

ext.lib = [
        scala           : "org.scala-lang:scala-library:${ver.scala}.4",
        sparkCore       : spark("core"),
        sparkStreaming  : spark("streaming"),
        sparkStreamKafka: spark("streaming-kafka-0-10"),
        sparkSqlKafka   : spark("sql-kafka-0-10"),
        kafkaConnectJson: "org.apache.kafka:connect-json:${ver.kafka}.0",
        sparkSql        : spark("sql"),
        jackson         : "com.fasterxml.jackson.module:jackson-module-scala_2.11:${ver.jackson}",
]


dependencies {
    compile(
            lib.scala,
            lib.sparkCore,
            lib.sparkStreaming,
            lib.sparkStreamKafka,
            lib.sparkSqlKafka,
            lib.jackson,
            lib.kafkaConnectJson,
            lib.sparkSql
    )
    testCompile 'junit:junit:4.12'
    testCompile 'org.scalatest:scalatest_2.11:3.0.5'
}


jar {
    doFirst {
        from {
            configurations.compile.collect {
                def has = configurations.compileOnly.contains(it)
                // Keep for debug: */ if(!has) println(" * " + it)

                if (has) []
                else it.isDirectory() ? it : zipTree(it)
            }
        }
    }
    exclude 'META-INF/*.RSA', 'META-INF/*.SF', 'META-INF/*.DSA'
    zip64 = true
}

task tar(dependsOn: ["jar"], type: Tar) {
    extension = "tar.gz"
    compression = Compression.GZIP
    from("build/libs") {
        include "**/*.jar"
    }
}

